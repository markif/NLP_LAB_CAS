{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d29ea3d-6c1a-4c1a-b4bb-bca7388641a7",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"400\" src=\"https://drive.google.com/thumbnail?id=1rPeHEqFWHJcauZlU82a4hXM10TUjmHxM&sz=s4000\" alt=\"FHNW Logo\">\n",
    "\n",
    "\n",
    "# How to Generate with LLMs\n",
    "\n",
    "by Fabian Märki\n",
    "\n",
    "## Summary\n",
    "The aim of this notebook is to illustrate available adjustment \"screws\" for generating content/text.\n",
    "\n",
    "## Links\n",
    "- [How to Generate](https://huggingface.co/blog/how-to-generate)\n",
    "- [Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
    "- [Text Generation](https://github.com/nlp-with-transformers/notebooks/blob/main/05_text-generation.ipynb)\n",
    "\n",
    "This notebook contains assigments: <font color='red'>Questions are written in red.</font>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/markif/NLP_LAB_CAS/blob/master/How_to_Generate_with_LLMs.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7cf032-9fa3-4e20-bb95-581399bf9ccf",
   "metadata": {},
   "source": [
    "The inputs to the generate method depend on the model. They are returned by the model’s preprocessor class, such as AutoTokenizer or AutoProcessor. You can learn more about the individual model’s preprocessor in the corresponding model’s documentation.\n",
    "\n",
    "The process of selecting output tokens to generate text is known as decoding, and you can customize the decoding strategy that the `generate()` method will use. Modifying a decoding strategy can have a noticeable impact on the quality of the generated output. It can help reduce repetition in the text and make it more coherent.\n",
    "\n",
    "[Text generation](https://huggingface.co/docs/transformers/v4.45.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) is essential to many NLP tasks:\n",
    "- [Text summarization](https://huggingface.co/docs/transformers/tasks/summarization#inference)\n",
    "- [Image captioning](https://huggingface.co/docs/transformers/model_doc/git#transformers.GitForCausalLM.forward.example)\n",
    "- [Audio transcription](https://huggingface.co/docs/transformers/model_doc/whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2036785d-e9a3-4afd-9fef-daa7a5a47d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# fix a compatibility issues among the libraries (type object 'LlamaForCausalLM' has no attribute 'transformers-community/group-beam-search')\n",
    "!pip install transformers==4.53.3\n",
    "\n",
    "!pip install accelerate\n",
    "!pip install optimum\n",
    "!pip install auto-gptq\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d595e3-3995-4294-86b4-f31dccbdb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install 'fhnw-nlp-utils>=0.11.0,<0.12.0'\n",
    "\n",
    "from fhnw.nlp.utils.storage import download\n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdc3e1-7a9a-4af5-9e5c-e946d8cdd39b",
   "metadata": {},
   "source": [
    "**Make sure that a GPU is available (see [here](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm))!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0969a2-229d-46b2-aa84-c956922a690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 10:15:33.426168: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-09 10:15:33.441139: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-09 10:15:33.445710: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS name: posix\n",
      "Platform name: Linux\n",
      "Platform release: 6.8.0-47-generic\n",
      "Python version: 3.11.0rc1\n",
      "CPU cores: 6\n",
      "RAM: 31.11GB total and 23.38GB available\n",
      "Tensorflow version: 2.17.0\n",
      "GPU is available\n",
      "GPU is a NVIDIA GeForce RTX 2070 with Max-Q Design with 8192MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pynvml/smi.py:5: FutureWarning: The pynvml.smi module is deprecated and will be removed in the next release of pynvml. Please use pynvml_utils:\n",
      "(e.g. `from pynvml_utils import nvidia_smi`)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from fhnw.nlp.utils.system import set_log_level\n",
    "from fhnw.nlp.utils.system import system_info\n",
    "\n",
    "set_log_level()\n",
    "print(system_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e12e9a6-e3cb-40f1-9331-a4e615092f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e501c266d04d0a807c66b06da987e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "llm = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True}\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8611f32-1dfc-4e5f-bbbb-0e8e88bace70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db3b0c8-1449-481f-87f4-28201f604283",
   "metadata": {},
   "source": [
    "A decoding strategy for a model is defined in its generation configuration. When using pre-trained models for inference within a `pipeline()`, the models call the `PreTrainedModel.generate()` method that applies a default generation configuration under the hood. The default configuration is also used when no custom configuration has been saved with the model.\n",
    "\n",
    "When you load a model, you can inspect the generation configuration that comes with it through `model.generation_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05f76b7-f35b-4b1a-85b3-4a456a796825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    128001,\n",
       "    128008,\n",
       "    128009\n",
       "  ],\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e440f9e2-3f4d-4886-9661-b8d5672072f6",
   "metadata": {},
   "source": [
    "You can override any generation_config by passing the parameters and their values directly to the generate method:\n",
    "\n",
    "```python\n",
    "llm(prompt, num_beams=4, do_sample=True)\n",
    "```\n",
    "\n",
    "or you can save your custom decoding strategy:\n",
    "\n",
    "```python\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50, do_sample=True, top_k=50\n",
    ")\n",
    "generation_config.save_pretrained(\"model_folder\")\n",
    "\n",
    "...\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(\"model_folder\")\n",
    "generate_kwargs = {\n",
    "    \"generation_config\": generation_config,\n",
    "}\n",
    "\n",
    "llm = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    generate_kwargs=generate_kwargs,\n",
    ")\n",
    "```\n",
    "\n",
    "Even if the default decoding strategy works for your task, you can still tweak a few things. Some of the commonly adjusted parameters include:\n",
    "\n",
    "- `max_new_tokens`: the maximum number of tokens to generate. In other words, the size of the output sequence, not including the tokens in the prompt. As an alternative to using the output’s length as a stopping criteria, you can choose to stop generation whenever the full generation exceeds some amount of time. To learn more, check StoppingCriteria.\n",
    "- `num_beams`: by specifying a number of beams higher than 1, you are effectively switching from greedy search to beam search. This strategy evaluates several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that start with a lower probability initial tokens and would’ve been ignored by the greedy search. Visualize how it works here.\n",
    "- `do_sample`: if set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.\n",
    "- `num_return_sequences`: the number of sequence candidates to return for each input. This option is only available for the decoding strategies that support multiple sequence candidates, e.g. variations of beam search and sampling. Decoding strategies like greedy search and contrastive search return a single output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51712aa-7a9c-46d9-8dad-03c80dbee944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# for reproducibility\n",
    "set_seed(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bcd04-c373-47de-8fe0-2b8d2357aea4",
   "metadata": {},
   "source": [
    "Let's get started and build or first chat (template)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26988b3-08b5-436c-989d-c9b22be5099f",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK: Get an understanding of the differences between `system` and `user` prompts (e.g. by consulting [this](https://www.regie.ai/blog/user-prompts-vs-system-prompts) resource).**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd3737-0b7e-494c-bfc0-839f464f7d98",
   "metadata": {},
   "source": [
    "<font color='green'>**System prompts** ...</font>\n",
    "\n",
    "<font color='green'>**User prompts** ...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "486c220d-776c-4b09-b0df-b56934f91bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a skilled Python developer specializing in database management and optimization.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I am experiencing a sorting issue in my database. Could you please provide Python code to help resolve this problem?\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff229d10-d52a-4a53-a2f2-c0bdf9cc85c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9045a44-7bf0-4581-a0d0-f1a64a2ed2be",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK: Go to [Chat Template](https://huggingface.co/docs/transformers/chat_templating) and get an understanding on how to process/handle chat templates within the Huggingface ecosystem.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1f5e0-7be4-4910-9588-6dec5b364c59",
   "metadata": {},
   "source": [
    "Let's check if it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4256dcff-4381-4243-bb5b-a299a60b4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(outputs):\n",
    "    return [output[\"generated_text\"].split(\"<|start_header_id|>assistant<|end_header_id|>\")[1] for output in outputs]\n",
    "\n",
    "def get_response(outputs):\n",
    "    return get_responses(outputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06b60e0-d314-4d4e-a5c6-cab833cd9868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "I'd be happy to help you resolve your database sorting issue. However, I'll need more information about the problem you're experiencing.\n",
       "\n",
       "To better assist you, could you please provide more details about your database setup, the data you're trying to sort, and the specific issue you're facing? This will help me provide a more accurate and effective solution.\n",
       "\n",
       "Here are some questions to consider:\n",
       "\n",
       "1. What type of database are you using (e.g., MySQL, PostgreSQL, MongoDB, SQLite)?\n",
       "2. What is the structure of your database tables?\n",
       "3. What data are you trying to sort (e.g., dates, integers, strings)?\n",
       "4. What is the specific issue you're experiencing (e.g., slow performance, incorrect sorting order, inconsistent results)?\n",
       "\n",
       "Once I have this information, I can provide you with a Python code snippet to help resolve your sorting issue.\n",
       "\n",
       "If you're working with a specific database library, such as `pandas` or `sqlite3`, I can provide code examples using those libraries.\n",
       "\n",
       "Here's an example of a basic sorting function using the `pandas` library:\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "# create a sample DataFrame\n",
       "data = {\n",
       "    'Name': ['John', 'Alice', 'Bob', 'Eve'],\n",
       "    'Age': [25, 30, 35, 20]\n",
       "}\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# sort by 'Age' column in ascending order\n",
       "df_sorted = df.sort_values(by='Age')\n",
       "\n",
       "print(df_sorted)\n",
       "```\n",
       "\n",
       "This code creates a sample DataFrame, sorts it by the 'Age' column in ascending order, and prints the result.\n",
       "\n",
       "If you're using a different database library or have more complex sorting requirements, please provide more details, and I'll do my best to assist you with a Python code solution."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "prompt = llm.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "display(Markdown(get_response(outputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cad703-c09b-4a29-87f2-728cbe12409d",
   "metadata": {},
   "source": [
    "Alternatively we could also directly call..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63cf572c-209c-4334-bf56-a44e69de4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'd be happy to help you with your database sorting issue.\n",
       "\n",
       "Before I provide you with code, could you please provide more information about your problem? This will help me better understand your requirements and provide a more accurate solution.\n",
       "\n",
       "Here are some details that would be helpful to know:\n",
       "\n",
       "1. What type of database are you using (e.g., MySQL, PostgreSQL, SQLite)?\n",
       "2. What is the data type of the column you're trying to sort by?\n",
       "3. What is the current state of your data (e.g., is it sorted, unsorted, or partially sorted)?\n",
       "4. What is the desired order of your data (e.g., ascending, descending, alphabetical)?\n",
       "5. Are there any specific constraints or requirements that I should be aware of?\n",
       "\n",
       "Assuming you're using a Python database library such as `sqlite3` or `pandas`, here's an example code snippet that demonstrates how to sort data in a database:\n",
       "\n",
       "**Sorting Data in a SQLite Database**\n",
       "```python\n",
       "import sqlite3\n",
       "\n",
       "# Connect to the database\n",
       "conn = sqlite3.connect('my_database.db')\n",
       "\n",
       "# Create a cursor object\n",
       "cur = conn.cursor()\n",
       "\n",
       "# Define the table name and column to sort by\n",
       "table_name ='my_table'\n",
       "sort_column = 'id'\n",
       "\n",
       "# Sort the data in ascending order\n",
       "cur.execute(f\"SELECT * FROM {table_name} ORDER BY {sort_column} ASC\")\n",
       "\n",
       "# Fetch the sorted data\n",
       "sorted_data = cur.fetchall()\n",
       "\n",
       "# Print the sorted data\n",
       "for row in sorted_data:\n",
       "    print(row)\n",
       "\n",
       "# Close the connection\n",
       "conn.close()\n",
       "```\n",
       "This code snippet demonstrates how to sort data in a SQLite database by a specified column. You can modify the code to suit your specific requirements.\n",
       "\n",
       "If you're using a Pandas DataFrame, you can use the following code to sort data:\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "# Load the data into a DataFrame\n",
       "df = pd.read_csv('my_data.csv')\n",
       "\n",
       "# Sort the data in ascending order\n",
       "df_sorted = df.sort_values(by='id')\n",
       "\n",
       "# Print the sorted data\n",
       "print(df_sorted)\n",
       "```\n",
       "Please provide more information about your problem, and I'll be happy to assist you further!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm(messages, max_new_tokens=512, do_sample=True)[0]['generated_text'][-1]\n",
    "\n",
    "display(Markdown(outputs[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533694b0-6873-494f-a37b-1365f5f85886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "949ab1be-1c01-4057-b6b9-d8115a51825a",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK: Decide upon a prompt (you might want to define your own).**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87049c3b-9892-4178-ade1-ff27d4cc634c",
   "metadata": {},
   "source": [
    "In case you are interested: Some details about the [Chat Markup Language (ChatML)](https://cobusgreyling.medium.com/openai-introduced-chat-markup-language-chatml-based-input-to-non-chat-modes-6ca4b267012f) displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51cda88f-f053-4a26-be5c-dfeda8835921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are skilled story teller experienced in writing funny jokes.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a joke about an NLP class that does not pay attention to their lecturer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are skilled story teller experienced in writing funny jokes.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me a joke about an NLP class that does not pay attention to their lecturer.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = llm.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd370ad-499a-4e5a-a7df-0293fcff25df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a skilled writer experienced in writing emails.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Write a email to schedule a meeting with your colleague John Doe to disucss the requirements for the RAG application. Ask him when he is free.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a skilled writer experienced in writing emails.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Write a email to schedule a meeting with your colleague John Doe to disucss the requirements for the RAG application. \"\n",
    "            \"Ask him when he is free. \" \n",
    "            # we might want to add some instructions to keep the email short but to the sake of the task we skip this\n",
    "            #\"Keep the email short. \"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = llm.tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d900346-ed74-4b33-86e5-a13ff455ea1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55eaa096-477a-4f50-8564-3411542de11a",
   "metadata": {},
   "source": [
    "<font color='red'>**TASK (ongoing): Play with the parameters.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05baa7-a4db-4d30-8abb-ef55a5fefe01",
   "metadata": {},
   "source": [
    "## Decoding Strategies\n",
    "\n",
    "Certain combinations of the `generate()` parameters, and ultimately generation_config, can be used to enable specific decoding strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17fb1a-dd0e-4f3b-a211-f20bced144c7",
   "metadata": {},
   "source": [
    "### Greedy Search\n",
    "\n",
    "Greedy search decoding is the default generation strategy. This means the parameters are set to `num_beams=1` and `do_sample=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d02d584-d7c9-41a6-ae9e-1f18f919b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a1ef51-07fd-403d-9694-444303010e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is a neutral and professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a time that suits you?\n",
      "\n",
      "I look forward to hearing back from you and discussing the details of the project.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 6.19 s, sys: 59.3 ms, total: 6.25 s\n",
      "Wall time: 6.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=False, num_beams=1)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee094be2-932d-456d-a445-5a3b93c18984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d53fb1db-83bf-41e9-8550-47b43ebf5355",
   "metadata": {},
   "source": [
    "### Beam-Search Decoding\n",
    "\n",
    "Unlike greedy search, beam-search decoding keeps several hypotheses at each time step and eventually chooses the hypothesis that has the overall highest probability for the entire sequence. This has the advantage of identifying high-probability sequences that start with lower probability initial tokens and would’ve been ignored by the greedy search. Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n",
    "\n",
    "Use this [demo](https://huggingface.co/spaces/m-ric/beam_search_visualizer) to visualize how beam-search decoding works.\n",
    "\n",
    "To enable beam-search decoding set `num_beams` (aka number of hypotheses to keep track of) to a number that is greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f38e169f-2a4f-48ab-8e73-5d44826869a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, either in person or virtually, and can accommodate your schedule.\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 45.4 s, sys: 7.78 s, total: 53.2 s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=False, num_beams=3, early_stopping=True)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5c567-621f-434a-9670-d8ca404a832c",
   "metadata": {},
   "source": [
    "Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best.\n",
    "\n",
    "To enable this feature set the parameter `num_return_sequences` to the number of highest scoring beams that should be returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34642c05-a508-48b8-a794-041895769b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0: \n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, either in person or virtually, and can accommodate your schedule.\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "1: \n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, either in person or virtually, and can accommodate your schedule.\n",
      "\n",
      "Looking forward to hearing back from you and discussing the RAG application requirements.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "2: \n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, either in person or virtually, and can accommodate your schedule.\n",
      "\n",
      "Looking forward to hearing back from you and discussing the requirements in more detail.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 45.6 s, sys: 7.85 s, total: 53.5 s\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=False, num_beams=3, early_stopping=True, num_return_sequences=3)\n",
    "\n",
    "print(100 * '-')\n",
    "for i, text in enumerate(get_responses(outputs)):\n",
    "  print(\"{}: {}\".format(i, text))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765b8d4-dd71-44b9-a101-f1fc13193e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "748caa50-832a-4345-8e38-921f04d7276a",
   "metadata": {},
   "source": [
    "### Diverse Beam-Search Decoding\n",
    "\n",
    "Diverse beam search decoding strategy is an extension of the beam search strategy that allows for generating a more diverse set of beam sequences to choose from. \n",
    "\n",
    "This approach has three main parameters: `num_beams`, `num_beam_groups`, and `diversity_penalty`. The diversity penalty ensures the outputs are distinct across groups, and beam search is used within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd52791a-97b1-470c-ab26-ae16244b7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0: \n",
      "\n",
      "Here is a neutral and professional email:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a time that suits you?\n",
      "\n",
      "I look forward to hearing back from you and discussing the details.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "1: \n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I am reaching out to schedule a meeting with you to discuss the requirements for the RAG application. I would appreciate it if you could let me know your availability over the next few days, and we can schedule a meeting at a time that suits you.\n",
      "\n",
      "Please reply with your preferred dates and times, and I will make sure to accommodate them.\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "2: \n",
      "\n",
      "Here's an email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I'd like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a time that suits you?\n",
      "\n",
      "I'm looking forward to hearing back from you and discussing the details.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 41.2 s, sys: 6.49 s, total: 47.7 s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=False, num_beams=3, early_stopping=True, num_return_sequences=3, num_beam_groups=3, diversity_penalty=5.0)\n",
    "\n",
    "print(100 * '-')\n",
    "for i, text in enumerate(get_responses(outputs)):\n",
    "  print(\"{}: {}\".format(i, text))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d87e0d-68f6-4e8b-9822-2869955e620c",
   "metadata": {},
   "source": [
    "As can be seen, the beam hypotheses are only marginally different to each other - which should not be too surprising since we do not use sampling (only marginal diversity in the generated text).\n",
    "\n",
    "In open-ended generation, a couple of reasons have recently been brought forward why beam search might not be the best possible option:\n",
    "\n",
    "- Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization. But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n",
    "\n",
    "- Beam search can suffer from repetitive generation and this can be hard to control and can require a lot of finetuning (this was an issue with simple/early models like gpt-2 but not with LLMs anymore).\n",
    "\n",
    "- High quality human language does not follow a distribution of high probable next words. As humans, we want generated text to surprise us and not to be boring/predictable. The following figure shows this by plotting the probability a model would give to human text vs. what beam search does.\n",
    "\n",
    "<img src=\"https://drive.switch.ch/index.php/s/QAe46FquKEqDlYL/download\" alt=\"System vs. User prompt\">\n",
    "\n",
    "So let's introduce some randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56767ab5-b46e-4942-bc54-97872ffe94f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89405060-8e7a-4fa2-8960-05e43e35ca82",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "In its most basic form, sampling means randomly picking the next word according to its conditional probability distribution:\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "It becomes obvious that language generation using sampling is not deterministic anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e815ea9-7abe-4b26-bfe4-68bd8d53208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "# for reproducibility\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f17bd-c0ce-4081-8fdc-23788d4757d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62d63d83-de27-43b6-8ea9-e0047024e5c0",
   "metadata": {},
   "source": [
    "### Multinomial Sampling\n",
    "Multinomial sampling (also called ancestral sampling) randomly selects the next token based on the probability distribution over the entire vocabulary of the model (opposed to greedy search that always chooses a token with the highest probability as the next token). Every token with a non-zero probability has a chance of being selected, thus reducing the risk of repetition.\n",
    "\n",
    "To enable multinomial sampling set `num_beams=1` and `do_sample=True` (and deactivate *top-k* sampling (more on this later) via `top_k=0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42eb31c5-a543-46c8-849f-6fee50f7aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I am reaching out to schedule a meeting with you to discuss the requirements for the RAG application. I would appreciate the opportunity to touch base with you to ensure we are on the same page regarding the project's specifications and timelines.\n",
      "\n",
      "Would you be available to meet at your earliest convenience? Could you please let me know your availability, and I will schedule a meeting at a time that suits you?\n",
      "\n",
      "Thank you for your time, and I look forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 8.36 s, sys: 83.4 ms, total: 8.45 s\n",
      "Wall time: 8.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, num_beams=1, top_k=0)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d1511-0d8f-4df7-be3f-f792b7912f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74d5afea-7d76-4fe5-84a7-89a3ad6c9b78",
   "metadata": {},
   "source": [
    "### Beam-Search Multinomial Sampling\n",
    "\n",
    "This decoding strategy combines beam search with multinomial sampling. \n",
    "\n",
    "To enable this strategy you need to specify the `num_beams` greater than 1 and `do_sample=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca4f9b3e-43fd-4c3c-a4fa-fce51626b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0: \n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days, and we can schedule a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, either in person or virtually. Please reply to this email with a few dates and times that work for you, and I will do my best to accommodate your schedule.\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "1: \n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days, and we can schedule a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, either in person or virtually. Please reply to this email with a few dates and times that work for you, and I will do my best to accommodate your schedule.\n",
      "\n",
      "Looking forward to hearing back from you and discussing the RAG application requirements.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "2: \n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days, and we can schedule a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, either in person or virtually. Please reply to this email with your preferred date and time, and I will make sure to accommodate it.\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 54.6 s, sys: 9.31 s, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, num_beams=3, early_stopping=True, num_return_sequences=3, top_k=0)\n",
    "\n",
    "print(100 * '-')\n",
    "for i, text in enumerate(get_responses(outputs)):\n",
    "  print(\"{}: {}\".format(i, text))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc51ca-756e-4698-907a-90dc7bb385a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab8b6e53-1056-4720-ad4a-54cf1e96f85b",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "The `temperature` parameter in LLMs directly affects the variability and randomness of generated responses. A lower LLM `temperature` value (close to 0) produces more deterministic and focused outputs, ideal for tasks requiring factual accuracy, such as summarization or translation. Conversely, a higher `temperature` value (e.g., 1.0 or above) introduces more diversity and creativity, as the model samples from a broader range of possible words. This makes higher temperatures suitable for creative writing.\n",
    "\n",
    "That said, a lower `temperature` makes the distribution $P(w|w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the `temperature` of the [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max). \n",
    "\n",
    "For applications needing highly reliable and reproducible outputs, setting a lower LLM `temperature` (e.g., 0.2-0.3) helps maintain quality and reduces unexpected responses. In contrast, interactive applications like chatbots (or any content creation task) can benefit from a mid-range `temperature` (e.g., 0.7-0.9), allowing the model to be engaging without straying too far from meaningful responses.\n",
    "\n",
    "By setting `temperature` $ \\to 0$, temperature scaled sampling becomes equal to greedy decoding.\n",
    "\n",
    "<img src=\"https://drive.switch.ch/index.php/s/B6a30e8WbLbmwx8/download\" alt=\"Temperature\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a38a51f-3b53-459f-b196-9b6b87d115e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is a neutral and concise email:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability and suggest a few dates and times that work for you?\n",
      "\n",
      "I look forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 4.84 s, sys: 73.8 ms, total: 4.91 s\n",
      "Wall time: 4.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.1, top_k=0)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f78f1-9ad7-4ddc-868b-ce72d06d5968",
   "metadata": {},
   "source": [
    "If we set temperature even higher the generated text becomes gibberish...\n",
    "\n",
    "Additionally, it can happen that the LLM starts to make things up (I had an example where it wrote \"discuss the requirements for the **Results Affiliate Group (RAG)** application\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed61083f-02d7-41fd-bbc0-6f51cdeb8439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is an email:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I am writing to schedule a meeting to discuss the requirements for our bid on the Rao Application Group (RAG) application. Your insight and expertise will be invaluable to this process, and I look forward to discussing this further with you.\n",
      "\n",
      "Could you please let me know when your availability is the next few days so we can arrange a time that suits you?\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 6.96 s, sys: 73.4 ms, total: 7.03 s\n",
      "Wall time: 7.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=1.5, top_k=0)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1952116-ae96-49b4-8c2c-9d669c33133f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f51084-578b-4bb2-ad2b-12eebc6340c6",
   "metadata": {},
   "source": [
    "### Top-K Sampling\n",
    "\n",
    "In *top-k* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words. \n",
    "\n",
    "To enable top-k sampling set `top_k` to the number of words you want to consider from the probability distribution for the next word in the sequence.\n",
    "\n",
    "<img src=\"https://drive.switch.ch/index.php/s/3FxJ7e3r74zVy17/download\" alt=\"top-k vs. top-p\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f84caedb-2072-46b5-ab53-0f7925b6f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is a simple email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability this week or next so we can schedule a time that suits you?\n",
      "\n",
      "I look forward to hearing back from you and discussing the requirements in more detail.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 6.13 s, sys: 80 ms, total: 6.21 s\n",
      "Wall time: 6.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, top_k=20)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41638b8a-5417-4366-bbd6-8a97ff3e148f",
   "metadata": {},
   "source": [
    "One concern with *top-k* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$.\n",
    "This can be problematic as some words might be sampled from a very sharp distribution, whereas others from a much more flat distribution.\n",
    "\n",
    "Thus, limiting the sample pool to a fixed size *K* could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b032c92-ef9a-4558-9e37-28edd9b514e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f93df3fd-fc57-4a28-adfa-42f28b4c413e",
   "metadata": {},
   "source": [
    "### Top-P (Nucleus) Sampling\n",
    "\n",
    "Instead of sampling only from the most likely *K* words, in *top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. The probability mass is then redistributed among this set of words. This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c94c3247-34ae-4df0-9f7f-09de816a6f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is a professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting Invitation: RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a meeting at a time that suits you?\n",
      "\n",
      "I am available to meet at your convenience, whether that's in-person or remotely. Please reply to this email with your preferred date and time, and I will make sure to adjust my schedule accordingly.\n",
      "\n",
      "Thank you for your time, and I look forward to hearing back from you soon.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 8.8 s, sys: 83.8 ms, total: 8.88 s\n",
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, top_k=0, top_p=0.80)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac917c-f995-471a-ac13-3c16ce11ff89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb17a25-2576-49af-9a74-97c1d3d73271",
   "metadata": {},
   "source": [
    "### Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07146f77-7b8e-46e9-a8a4-0f85f0ffd161",
   "metadata": {},
   "source": [
    "While in theory, *top-p* seems more elegant than *top-k*, both methods work  well in practice. *top-p* can also be used in combination with *top-k*, which can avoid very low ranked words while allowing for some dynamic selection.\n",
    "\n",
    "To get multiple independently sampled outputs, we can *again* set the parameter `num_return_sequences > 1`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00df25f7-a60c-4aa5-a3cb-18c7c4db989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here is a neutral email:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I would like to schedule a meeting to discuss the requirements for the RAG application. Could you please let me know when you are available to meet?\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 3.67 s, sys: 80.6 ms, total: 3.75 s\n",
      "Wall time: 3.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, top_k=20, top_p=0.80)\n",
    "\n",
    "print(100 * '-')\n",
    "print(get_response(outputs))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4531d2-7b18-46a9-9a78-6591340e24e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c9e3711-e30e-423e-92ab-03bbb36cddf9",
   "metadata": {},
   "source": [
    "## Contrastive Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad569b08-5c18-4d92-a350-20a187eee492",
   "metadata": {},
   "source": [
    "See [here](https://huggingface.co/blog/introducing-csearch#5-contrastive-search) or [here](https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need) for a detailed explanation.\n",
    "\n",
    "Contrastive Search is a [method proposed in 2022](https://arxiv.org/pdf/2202.06417) that suppresses token repetition by making it more difficult to select token sequences that have been outputed previously. It suppresses repetitions more effectively than Beam Search with an equivalent amount of computation.\n",
    "\n",
    "To calculate the token $x_t$ at time $t$, the selection of $v$ from the token candidates $V^k$ is determined according to the following evaluation function. The terme `model confidence` is the probability value calculated by the model for the next tokens (i.e. the probability value of the token candidate $v$ given past tokens $x_{<t}$). In Greedy Search, the next token is determined only from this term.\n",
    "\n",
    "<img src=\"https://drive.switch.ch/index.php/s/RFuQ8gFdAU46xll/download\" alt=\"Contrastive Search Formula https://miro.medium.com/v2/resize:fit:720/format:webp/1*OEIwRq659xURfgrfGdWd1Q.png\">\n",
    "\n",
    "In `Contrastive Search`, a `degeneration penalty` term is added as a penalty. This penalty is calculated based on the past token sequence $x_j$ and the current candidate token $v$, and is subtracted from the probability value output by the model.\n",
    "\n",
    "In this context, $s$ represents the `cosine similarity` of the token’s hidden states. For each candidate token $v$, the model’s inference is performed again to obtain its hidden state. An 'cosine similarity' with past hidden states is calculated, and if the candidate token is semantically close to previous tokens, a higher penalty is applied.\n",
    "\n",
    "If the model’s inference is performed to calculate the hidden states for all candidates, the computational load becomes significant. Therefore, the penalty is calculated only for the `top_k` candidates with the highest probabilities.\n",
    "\n",
    "$α$ is a parameter used to control the penalty. If it is set to 0, the process is equivalent to Greedy Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c628901c-7fa3-4082-b55d-666594621a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0: \n",
      "\n",
      "Here is a neutral and professional email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next week so that we can schedule a time that suits you?\n",
      "\n",
      "I look forward to hearing back from you and discussing the details.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "1: \n",
      "\n",
      "Here is a neutral email to schedule a meeting:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I would like to schedule a meeting to discuss the requirements for the RAG application. Would you be available to meet at your convenience?\n",
      "\n",
      "Please let me know a time that suits you and I will ensure I am available.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "2: \n",
      "\n",
      "Here is a draft email:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well. I would like to schedule a meeting with you to discuss the requirements for the RAG application. Could you please let me know your availability over the next few days so we can schedule a meeting at your earliest convenience?\n",
      "\n",
      "I would appreciate it if you could suggest a few times that work for you, and I will do my best to accommodate your schedule.\n",
      "\n",
      "Looking forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 42.8 s, sys: 8.13 s, total: 50.9 s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, top_k=200, top_p=0.95, num_return_sequences=3, penalty_alpha=0.4)\n",
    "\n",
    "print(100 * '-')\n",
    "for i, text in enumerate(get_responses(outputs)):\n",
    "  print(\"{}: {}\".format(i, text))\n",
    "print(100 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b62ad2d-e70b-43f1-b8da-a2f1c4c483b7",
   "metadata": {},
   "source": [
    "When we increase temperature it can happen that the LLM starts to make things up (I had an example where it wrote \"I am available Monday to Friday, and I am flexible to meet at your preferred time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1736d12-1eb2-491f-8b91-6f27db59d076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0: \n",
      "\n",
      "Here is a neutral, yet effective email:\n",
      "\n",
      "Subject: Meeting Invitation: RAG Application Requirements Discussion\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I'd like to schedule a meeting to discuss the requirements for the RAG application. Would you be available to meet at your earliest convenience? Please let me know a date and time that suits you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "1: \n",
      "\n",
      "Here is an email to schedule a meeting with John Doe:\n",
      "\n",
      "Subject: Meeting Invitation - RAG Application Discussion\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope you're doing well. I wanted to touch base with you regarding the requirements for the RAG application. It would be helpful to discuss this further to ensure we're on the same page.\n",
      "\n",
      "Would you be available to meet this week or early next week to discuss the requirements? Could you please let me know a time that suits you?\n",
      "\n",
      "I look forward to hearing back from you.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "2: \n",
      "\n",
      "Here's a neutral email:\n",
      "\n",
      "Subject: Meeting to Discuss RAG Application Requirements\n",
      "\n",
      "Dear John,\n",
      "\n",
      "I hope this email finds you well.\n",
      "\n",
      "I would like to discuss the requirements for the RAG application. I believe it would be beneficial to have a meeting to go over the details. Do you have any availability in your schedule that we could meet?\n",
      "\n",
      "Please let me know when you're free, and I'll adjust my schedule accordingly.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "CPU times: user 42.1 s, sys: 7.95 s, total: 50.1 s\n",
      "Wall time: 50.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "outputs = llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=1.2, top_k=200, top_p=0.95, num_return_sequences=3, penalty_alpha=0.4)\n",
    "\n",
    "print(100 * '-')\n",
    "for i, text in enumerate(get_responses(outputs)):\n",
    "  print(\"{}: {}\".format(i, text))\n",
    "print(100 * '-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
